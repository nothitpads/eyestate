<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>EyeState</title>
  <style>
    :root{--bg:#f2f2f2;--card:#0f0f10;--muted:#9aa1a6;--accent:#e63946}
    html,body{height:100%;margin:0;font-family:Inter,system-ui,Segoe UI,Roboto,"Helvetica Neue",Arial}
    .wrap{min-height:100%;display:flex;flex-direction:column}
    header{background:linear-gradient(180deg,#ffffff 0,#f6f6f6 100%);padding:28px 36px;border-bottom:1px solid #e6e6e6}
    .brand{font-weight:600;letter-spacing:0.2px}
    .hero{padding:80px 36px 56px;display:flex;justify-content:center;align-items:center;flex-direction:column;text-align:center}
    .hero h1{font-size:40px;margin:0 0 12px;font-weight:500;color:#111}
    .hero p{color:var(--muted);max-width:820px;margin:0}
    .panel-dark{background:#0b0b0c;color:#e9e9ea;padding:56px 48px}
    .cards{display:flex;gap:20px;max-width:1100px;margin:24px auto}
    .card{background:#0f1011;padding:22px;border-radius:8px;flex:1;min-height:160px;box-shadow:0 4px 18px rgba(0,0,0,0.4)}
    .card h3{margin:0 0 12px;font-weight:600}
    .demo{background:white;padding:40px;display:flex;gap:32px;align-items:flex-start;justify-content:center}
    .controls{max-width:420px}
    .video-wrap{position:relative;width:640px;height:480px;background:#222;border-radius:6px;overflow:hidden}
    video{width:100%;height:100%;object-fit:cover}
    canvas.overlay{position:absolute;left:0;top:0;width:100%;height:100%;pointer-events:none}
    .status{margin-top:12px;color:#222;font-size:14px}
    .kpi{display:flex;gap:12px;margin-top:16px}
    .kpi .item{background:#fff;border:1px solid #eee;padding:10px 12px;border-radius:6px;min-width:92px;text-align:center}
    .footer{padding:20px 36px;color:var(--muted);font-size:13px}
    button{background:var(--card);color:#fff;border:0;padding:8px 12px;border-radius:6px;cursor:pointer}
    .muted{color:var(--muted)}
    .badge{display:inline-block;padding:6px 10px;border-radius:999px;background:#fff;color:#111;font-weight:600;border:1px solid #eee}
    .small{font-size:13px}
    a.link{color:var(--accent);text-decoration:none}
    @media (max-width:900px){.demo{flex-direction:column;align-items:center}.video-wrap{width:92vw;height:69vw}}
  </style>
</head>
<body>
  <div class="wrap">
    <header>
      <div style="display:flex;justify-content:space-between;align-items:center;max-width:1200px;margin:0 auto">
        <div class="brand">EyeState</div>
        <div class="small muted">Local deployment</div>
      </div>
    </header>

    <section class="hero">
      <h1>Real-time eye state detection</h1>
      <p>Minimal, privacy-first demo showing camera → eye state prediction → visual explanation (landmarks + heatmap).</p>
    </section>

    <section class="panel-dark">
      <div style="max-width:1200px;margin:0 auto">
        <h2 style="margin:0;color:#fff;font-weight:500">How the demo works</h2>
        <div class="cards">
          <div class="card">
            <h3>Live camera</h3>
            <div class="muted small">WebRTC feed captured from your browser.</div>
          </div>
          <div class="card">
            <h3>ML classifier</h3>
            <div class="muted small">ONNX model (exported from PyTorch `best_model.pth`) runs locally via ONNX Runtime Web. Produces logits → probability and a smoothed state.</div>
          </div>
        </div>
      </div>
    </section>

    <section class="demo">
      <div class="video-wrap" id="videoWrap">
        <video id="video" autoplay playsinline muted></video>
        <canvas id="overlay" class="overlay"></canvas>
      </div>

      <div class="controls">
        <div style="display:flex;gap:8px;align-items:center"><button id="startBtn">Start camera</button><button id="stopBtn" style="background:#e63946">Stop</button></div>
        <div class="status" id="status">Status: idle</div>

        <div class="kpi">
          <div class="item"><div class="small muted">FPS</div><div id="fps" style="font-weight:700">—</div></div>
          <div class="item"><div class="small muted">Latency (ms)</div><div id="lat" style="font-weight:700">—</div></div>
          <div class="item"><div class="small muted">Eye state</div><div id="eyeState" style="font-weight:700">—</div></div>
        </div>

        <div style="margin-top:18px" class="muted small">Model prob (smoothed): <span id="probVal">—</span></div>
        <div style="margin-top:6px" class="muted small">Threshold: <span id="thresholdVal">0.25</span></div>
        <input id="threshold" type="range" min="0.12" max="0.42" step="0.01" value="0.25" style="width:100%;margin-top:6px">

        <div style="margin-top:14px" class="muted small">Source: MediaPipe FaceMesh (landmarks) + ONNX classifier.</div>

      </div>
    </section>
  </div>

  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>

  <script>
    // ONNX runtime + helpers
    const PRED_SIZE = 80;               
    const PRED_INTERVAL_MS = 150;       
    let lastPredictTime = 0;
    let ortSession = null;
    window.temperature = 1.0;           
    let probSmooth = 0.5;
    const SMOOTH_ALPHA = 0.75;         

    async function initONNX(){
      try {
        ortSession = await ort.InferenceSession.create('eye_state.onnx');
        console.log('ONNX session ready');
      } catch (e) {
        console.warn('ONNX load failed (no model or network error). Running heuristic only.', e);
        ortSession = null;
      }
    }
    initONNX();

    // convert ImageData (RGBA) -> normalized CHW Float32Array
    function cropToTensorData(imgData, inH, inW){
      const data = imgData.data;
      const float32 = new Float32Array(3 * inH * inW);
      const mean = [0.485,0.456,0.406];
      const std = [0.229,0.224,0.225];
      let dst = 0;
      // imgData is row-major RGBA
      for (let c=0; c<3; c++){
        for (let y=0; y<inH; y++){
          for (let x=0; x<inW; x++){
            const px = (y * inW + x) * 4;
            const v = data[px + c] / 255.0;               // R=0,G=1,B=2
            float32[dst++] = (v - mean[c]) / std[c];
          }
        }
      }
      return float32;
    }

    // ONNX inference: returns {prob, logits, predIdx}
    async function runOnnxInfer(float32Array, H, W){
      if (!ortSession) throw new Error('ONNX session not available');
      const tensor = new ort.Tensor('float32', float32Array, [1,3,H,W]);
      const feeds = { input: tensor };
      const out = await ortSession.run(feeds);
      // output name 'logits' or first output
      const outKeys = Object.keys(out);
      const outArr = out[outKeys[0]].data;
      // apply temperature and convert to probability
      const T = window.temperature || 1.0;
      if (outArr.length === 1){
        const logit = outArr[0] / T;
        const prob = 1 / (1 + Math.exp(-logit));
        return { prob, logits: [outArr[0]], predIdx: prob > 0.5 ? 1 : 0 };
      } else if (outArr.length === 2){
        const l0 = outArr[0] / T, l1 = outArr[1] / T;
        const m = Math.max(l0,l1);
        const e0 = Math.exp(l0 - m), e1 = Math.exp(l1 - m);
        const s = e0 + e1;
        const p0 = e0 / s, p1 = e1 / s;
        const predIdx = p0 > p1 ? 0 : 1;
        const prob = predIdx === 0 ? p0 : p1;
        return { prob, logits: [outArr[0], outArr[1]], predIdx };
      } else {
        throw new Error('Unexpected ONNX output size: ' + outArr.length);
      }
    }
  </script>

  <script>
    // Network prediction (browser-side, uses ONNX session if loaded)
    function getCropCanvas(centerX, centerY, boxSizePx) {
      const vx = video.videoWidth, vy = video.videoHeight;
      const ow = overlay.width, oh = overlay.height;
      const sx = Math.max(0, (centerX - boxSizePx / 2) * (vx / ow));
      const sy = Math.max(0, (centerY - boxSizePx / 2) * (vy / oh));
      const sw = Math.min(vx - sx, boxSizePx * (vx / ow));
      const sh = Math.min(vy - sy, boxSizePx * (vy / oh));

      const c = document.createElement('canvas');
      c.width = PRED_SIZE; c.height = PRED_SIZE;
      const cctx = c.getContext('2d');
      cctx.drawImage(video, sx, sy, sw, sh, 0, 0, PRED_SIZE, PRED_SIZE);
      return c;
    }

    async function maybePredict(centerX, centerY, eyeDistancePx) {
      const now = Date.now();
      if (now - lastPredictTime < PRED_INTERVAL_MS) return;
      lastPredictTime = now;

      // preprocess crop
      const box = Math.max(eyeDistancePx * 3.0, 64);
      const cropCanvas = getCropCanvas(centerX, centerY, box);
      if (!ortSession){
        return;
      }

      try {
        const ctx2 = cropCanvas.getContext('2d');
        const imgData = ctx2.getImageData(0, 0, PRED_SIZE, PRED_SIZE);
        const float32 = cropToTensorData(imgData, PRED_SIZE, PRED_SIZE);
        const res = await runOnnxInfer(float32, PRED_SIZE, PRED_SIZE);
        // smoothing
        probSmooth = SMOOTH_ALPHA * probSmooth + (1 - SMOOTH_ALPHA) * res.prob;
        const pct = (probSmooth * 100).toFixed(1);
        // label index 0 = OPEN, 1 = CLOSED
        const labelMap = {0: 'OPEN', 1: 'CLOSED'};
        const mappedLabel = labelMap[res.predIdx] || (res.predIdx === 0 ? 'OPEN' : 'CLOSED');
        eyeStateEl.textContent = `${mappedLabel} (${pct}%)`;
        document.getElementById('probVal').textContent = `${pct}%`;
      } catch (err) {
        console.warn('ONNX infer error', err);
      }
    }
  </script>

  <script>
    // MediaPipe FaceMesh + EAR with ONNX hook
    const video = document.getElementById('video');
    const overlay = document.getElementById('overlay');
    const ctx = overlay.getContext('2d');
    const startBtn = document.getElementById('startBtn');
    const stopBtn = document.getElementById('stopBtn');
    const statusEl = document.getElementById('status');
    const fpsEl = document.getElementById('fps');
    const latEl = document.getElementById('lat');
    const eyeStateEl = document.getElementById('eyeState');
    const thr = document.getElementById('threshold');
    const thrVal = document.getElementById('thresholdVal');

    let camera = null;
    let lastFrameTime = performance.now();
    let frameCount = 0;
    let lastFpsTime = performance.now();
    let running = false;

    thr.addEventListener('input', ()=>{ thrVal.textContent = thr.value });

    function resizeOverlay(){ overlay.width = video.clientWidth; overlay.height = video.clientHeight; }

    function dist(a,b){ const dx = a.x - b.x; const dy = a.y - b.y; return Math.hypot(dx,dy); }

    function computeEAR(pts){
      const p1=pts[0], p2=pts[1], p3=pts[2], p4=pts[3], p5=pts[4], p6=pts[5];
      const A = dist(p2,p6);
      const B = dist(p3,p5);
      const C = dist(p1,p4);
      if (C===0) return 0;
      return (A + B) / (2.0 * C);
    }

    const faceMesh = new FaceMesh({locateFile:(file)=>`https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`});
    faceMesh.setOptions({maxNumFaces:1,refineLandmarks:true,minDetectionConfidence:0.6,minTrackingConfidence:0.5});
    faceMesh.onResults(onResults);

    function onResults(results){
      if(!running) return;
      const tStart = performance.now();
      resizeOverlay();
      ctx.clearRect(0,0,overlay.width,overlay.height);

      if(!results.multiFaceLandmarks || results.multiFaceLandmarks.length===0){
        statusEl.textContent = 'Status: no face detected'; eyeStateEl.textContent = '—'; return;
      }

      const landmarks = results.multiFaceLandmarks[0];
      const mapped = landmarks.map(p=>({x: p.x * overlay.width, y: p.y * overlay.height, z: p.z}));

      const leftIdx = [33,160,158,133,153,144];
      const rightIdx = [362,385,387,263,373,380];

      const leftPts = leftIdx.map(i=>({x: landmarks[i].x, y: landmarks[i].y}));
      const rightPts = rightIdx.map(i=>({x: landmarks[i].x, y: landmarks[i].y}));

      const leftEAR = computeEAR(leftPts);
      const rightEAR = computeEAR(rightPts);
      const ear = (leftEAR + rightEAR) / 2.0;

      const threshold = parseFloat(thr.value);
      const eyeOpen = ear > threshold;

      const leftCenter = { x: (mapped[leftIdx[0]].x + mapped[leftIdx[3]].x)/2, y: (mapped[leftIdx[1]].y + mapped[leftIdx[4]].y)/2 };
      const rightCenter = { x: (mapped[rightIdx[0]].x + mapped[rightIdx[3]].x)/2, y: (mapped[rightIdx[1]].y + mapped[rightIdx[4]].y)/2 };

      function drawBlob(c,center){
        const grd = ctx.createRadialGradient(center.x,center.y,2,center.x,center.y,60);
        if (eyeOpen){ grd.addColorStop(0,'rgba(90,200,120,0.18)'); grd.addColorStop(1,'rgba(90,200,120,0)'); }
        else { grd.addColorStop(0,'rgba(230,57,70,0.22)'); grd.addColorStop(1,'rgba(230,57,70,0)'); }
        ctx.fillStyle = grd; ctx.beginPath(); ctx.arc(center.x,center.y,60,0,Math.PI*2); ctx.fill();
      }
      drawBlob(ctx,leftCenter); drawBlob(ctx,rightCenter);

      ctx.strokeStyle = 'rgba(255,255,255,0.9)'; ctx.lineWidth = 1.2;
      function drawEyeContour(indices){
        ctx.beginPath();
        indices.forEach((i,idx)=>{ const p = mapped[i]; if(idx===0) ctx.moveTo(p.x,p.y); else ctx.lineTo(p.x,p.y); });
        ctx.closePath(); ctx.stroke();
        indices.forEach(i=>{ const p=mapped[i]; ctx.fillStyle='rgba(255,255,255,0.9)'; ctx.fillRect(p.x-1.5,p.y-1.5,3,3) });
      }
      drawEyeContour(leftIdx); drawEyeContour(rightIdx);

      ctx.fillStyle = 'rgba(0,0,0,0.45)'; ctx.fillRect(6,6,200,46);
      ctx.fillStyle = '#fff'; ctx.font='14px system-ui'; ctx.fillText('EAR: '+ear.toFixed(3),12,26);
      ctx.fillText('State: '+(eyeOpen? 'OPEN':'CLOSED'),12,44);

      if (!ortSession){
        eyeStateEl.textContent = eyeOpen ? 'OPEN' : 'CLOSED';
      }

      // request model prediction
      const midX = (leftCenter.x + rightCenter.x) / 2;
      const midY = (leftCenter.y + rightCenter.y) / 2;
      const eyeDistancePx = Math.hypot(leftCenter.x - rightCenter.x, leftCenter.y - rightCenter.y);
      maybePredict(midX, midY, eyeDistancePx);

      // FPS & latency
      frameCount++;
      const now = performance.now();
      const latency = now - tStart; latEl.textContent = latency.toFixed(1);
      const dt = now - lastFrameTime; lastFrameTime = now;
      if (now - lastFpsTime > 500){
        const fps = (frameCount*1000)/(now - lastFpsTime);
        fpsEl.textContent = Math.round(fps);
        frameCount = 0; lastFpsTime = now;
      }
    }

    // camera start/stop
    startBtn.addEventListener('click', async ()=>{
      if (running) return;
      try{
        const stream = await navigator.mediaDevices.getUserMedia({video:{width:640,height:480}, audio:false});
        video.srcObject = stream;
        running = true;
        statusEl.textContent = 'Status: starting';
        await new Promise(res=>video.onloadedmetadata = res);
        resizeOverlay();
        camera = new Camera(video, {onFrame: async ()=>{ await faceMesh.send({image: video}); }, width: video.videoWidth, height: video.videoHeight});
        camera.start();
        statusEl.textContent = 'Status: running';
      }catch(e){
        statusEl.textContent = 'Status: camera error';
        console.error(e);
      }
    });

    stopBtn.addEventListener('click', ()=>{
      if (!running) return;
      running = false;
      if (camera) camera.stop();
      const tracks = video.srcObject ? video.srcObject.getTracks() : [];
      tracks.forEach(t=>t.stop());
      video.srcObject = null;
      ctx.clearRect(0,0,overlay.width,overlay.height);
      statusEl.textContent = 'Status: stopped';
      fpsEl.textContent = '—'; latEl.textContent = '—'; eyeStateEl.textContent = '—'; document.getElementById('probVal').textContent = '—';
    });

    window.addEventListener('resize', ()=>{ if (video.videoWidth) resizeOverlay(); });
  </script>
</body>
</html>
